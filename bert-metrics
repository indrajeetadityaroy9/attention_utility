
import argparse
import json
import numpy as np
import pandas as pd
import glob
from bert_attention.core import metrics, information_flow

def load_data(csv_files):
    if len(csv_files) == 1:
        return pd.read_csv(csv_files[0])
    else:
        dfs = [pd.read_csv(f) for f in csv_files]
        return pd.concat(dfs, ignore_index=True)

def cmd_entropy(args):
    df = load_data(args.input)

    if args.per_head:
        results = {}
        for (layer, head), group in df.groupby(['layer', 'head']):
            entropy_dict = metrics.compute_entropy(group)
            avg_entropy = sum(entropy_dict.values()) / len(entropy_dict)
            results[f"L{layer}_H{head}"] = {
                'layer': int(layer),
                'head': int(head),
                'entropy': float(avg_entropy),
                'per_position': {int(k): float(v) for k, v in entropy_dict.items()}
            }
    else:
        entropy_dict = metrics.compute_entropy(df)
        avg_entropy = sum(entropy_dict.values()) / len(entropy_dict)
        results = {'entropy': float(avg_entropy)}

    with open(args.output, 'w') as f:
        json.dump(results, f, indent=2)

    print(f" Computed entropy, saved to {args.output}")

def cmd_special_ratio(args):
    df = load_data(args.input)

    special_tokens = args.special_tokens.split(',') if args.special_tokens else None

    if args.per_head:
        results = {}
        for (layer, head), group in df.groupby(['layer', 'head']):
            ratio = metrics.compute_special_token_ratio(group, special_tokens)
            results[f"L{layer}_H{head}"] = {
                'layer': int(layer),
                'head': int(head),
                'special_ratio': float(ratio)
            }
    else:
        ratio = metrics.compute_special_token_ratio(df, special_tokens)
        results = {'special_ratio': float(ratio)}

    with open(args.output, 'w') as f:
        json.dump(results, f, indent=2)

    print(f" Computed special token ratio, saved to {args.output}")

def cmd_distance(args):
    df = load_data(args.input)

    if args.per_head:
        results = {}
        for (layer, head), group in df.groupby(['layer', 'head']):
            dist = metrics.compute_attention_distance(group)
            results[f"L{layer}_H{head}"] = {
                'layer': int(layer),
                'head': int(head),
                'distance': float(dist)
            }
    else:
        dist = metrics.compute_attention_distance(df)
        results = {'distance': float(dist)}

    with open(args.output, 'w') as f:
        json.dump(results, f, indent=2)

    print(f" Computed attention distance, saved to {args.output}")

def cmd_sparsity(args):
    df = load_data(args.input)

    if args.per_head:
        results = {}
        for (layer, head), group in df.groupby(['layer', 'head']):
            sp = metrics.compute_sparsity(group, threshold=args.threshold)
            results[f"L{layer}_H{head}"] = {
                'layer': int(layer),
                'head': int(head),
                'sparsity': float(sp)
            }
    else:
        sp = metrics.compute_sparsity(df, threshold=args.threshold)
        results = {'sparsity': float(sp)}

    with open(args.output, 'w') as f:
        json.dump(results, f, indent=2)

    print(f" Computed sparsity, saved to {args.output}")

def cmd_flow(args):
    df = load_data(args.input)

    attention_matrices = []
    for layer in sorted(df['layer'].unique()):
        layer_df = df[df['layer'] == layer]

        if 'head' in layer_df.columns:
            pivot = layer_df.groupby(['from_token_idx', 'to_token_idx'])['attention_weight'].mean().reset_index()
        else:
            pivot = layer_df

        seq_len = max(pivot['from_token_idx'].max(), pivot['to_token_idx'].max()) + 1
        matrix = np.zeros((seq_len, seq_len))

        for _, row in pivot.iterrows():
            i, j = int(row['from_token_idx']), int(row['to_token_idx'])
            matrix[i, j] = row['attention_weight']

        row_sums = matrix.sum(axis=1, keepdims=True)
        matrix = matrix / (row_sums + 1e-9)

        attention_matrices.append(matrix)

    flow_result = information_flow.compute_attention_flow(
        attention_matrices,
        source_idx=args.source,
        target_idx=args.target
    )

    with open(args.output, 'w') as f:
        json.dump(flow_result, f, indent=2)

    print(f" Computed attention flow from token {args.source} to {args.target}")
    print(f"  Max flow: {flow_result['max_flow']:.4f}")
    print(f"  Bottleneck layers: {flow_result['bottleneck_layers']}")
    print(f"  Saved to {args.output}")

def cmd_markov(args):
    df = load_data(args.input)

    if args.layer is not None:
        df = df[df['layer'] == args.layer]

    if 'head' in df.columns:
        pivot = df.groupby(['from_token_idx', 'to_token_idx'])['attention_weight'].mean().reset_index()
    else:
        pivot = df

    seq_len = max(pivot['from_token_idx'].max(), pivot['to_token_idx'].max()) + 1
    attention_matrix = np.zeros((seq_len, seq_len))

    for _, row in pivot.iterrows():
        i, j = int(row['from_token_idx']), int(row['to_token_idx'])
        attention_matrix[i, j] = row['attention_weight']

    markov_result = information_flow.compute_markov_steady_state(attention_matrix)

    tokens = []
    if 'from_token' in df.columns:
        token_df = df[['from_token_idx', 'from_token']].drop_duplicates().sort_values('from_token_idx')
        tokens = token_df['from_token'].tolist()

    output_data = {
        'steady_state': markov_result['steady_state'].tolist(),
        'convergence': bool(markov_result['convergence']),
        'layer': args.layer if args.layer is not None else 'all'
    }

    if tokens:
        output_data['token_importance'] = [
            {'token': token, 'importance': float(importance)}
            for token, importance in zip(tokens, markov_result['steady_state'])
        ]

    with open(args.output, 'w') as f:
        json.dump(output_data, f, indent=2)

    print(f" Computed Markov steady-state distribution")
    print(f"  Convergence: {markov_result['convergence']}")
    if tokens:
        top_tokens = sorted(zip(tokens, markov_result['steady_state']), key=lambda x: x[1], reverse=True)[:5]
        print(f"  Top important tokens:")
        for token, importance in top_tokens:
            print(f"    {token}: {importance:.4f}")
    print(f"  Saved to {args.output}")

def cmd_all(args):
    df = load_data(args.input)

    if args.per_head:
        results = []
        for (layer, head), group in df.groupby(['layer', 'head']):
            head_metrics = metrics.compute_all_metrics(group)

            if isinstance(head_metrics.get('entropy'), dict):
                head_metrics['entropy'] = sum(head_metrics['entropy'].values()) / len(head_metrics['entropy'])

            head_metrics['layer'] = int(layer)
            head_metrics['head'] = int(head)

            results.append(head_metrics)

        output_data = {
            'metadata': {
                'num_layers': int(df['layer'].max() + 1),
                'num_heads': int(df['head'].max() + 1),
                'total_records': len(df)
            },
            'metrics_per_head': results
        }
    else:
        output_data = metrics.compute_all_metrics(df)
        if isinstance(output_data.get('entropy'), dict):
            output_data['entropy'] = sum(output_data['entropy'].values()) / len(output_data['entropy'])

    if args.format == 'json':
        with open(args.output, 'w') as f:
            json.dump(output_data, f, indent=2)
        print(f" Computed all metrics, saved to {args.output}")
    elif args.format == 'csv':
        if args.per_head:
            df_out = pd.DataFrame(output_data['metrics_per_head'])
            df_out.to_csv(args.output, index=False)
            print(f" Computed all metrics for {len(df_out)} heads, saved to {args.output}")
        else:
            df_out = pd.DataFrame([output_data])
            df_out.to_csv(args.output, index=False)
            print(f" Computed all metrics, saved to {args.output}")

def main():
    parser = argparse.ArgumentParser(
        description="Compute metrics from BERT attention data",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  bert-metrics entropy data.csv --output entropy.json --per-head

  bert-metrics special-ratio data.csv --output ratio.json

  bert-metrics distance data.csv --output distance.json --per-head

  bert-metrics all data.csv --output metrics.csv --format csv --per-head

  bert-metrics all data/*.csv --output aggregated.json --per-head
        """
    )

    parser.add_argument('--per-head', action='store_true',
                       help='Compute metrics per layer/head (default: overall)')

    subparsers = parser.add_subparsers(dest='command', required=True)

    entropy_parser = subparsers.add_parser('entropy', help='Compute Shannon entropy')
    entropy_parser.add_argument('input', nargs='+', help='Input CSV file(s)')
    entropy_parser.add_argument('--output', '-o', required=True, help='Output JSON file')

    special_parser = subparsers.add_parser('special-ratio', help='Compute special token attention ratio')
    special_parser.add_argument('input', nargs='+', help='Input CSV file(s)')
    special_parser.add_argument('--special-tokens', default='[CLS],[SEP],[PAD]',
                               help='Comma-separated special tokens (default: [CLS],[SEP],[PAD])')
    special_parser.add_argument('--output', '-o', required=True, help='Output JSON file')

    distance_parser = subparsers.add_parser('distance', help='Compute attention distance')
    distance_parser.add_argument('input', nargs='+', help='Input CSV file(s)')
    distance_parser.add_argument('--output', '-o', required=True, help='Output JSON file')

    sparsity_parser = subparsers.add_parser('sparsity', help='Compute attention sparsity')
    sparsity_parser.add_argument('input', nargs='+', help='Input CSV file(s)')
    sparsity_parser.add_argument('--threshold', type=float, default=1e-6,
                                help='Threshold for zero weights (default: 1e-6)')
    sparsity_parser.add_argument('--output', '-o', required=True, help='Output JSON file')

    flow_parser = subparsers.add_parser('flow', help='Compute attention flow (max flow between tokens)')
    flow_parser.add_argument('input', nargs='+', help='Input CSV file(s)')
    flow_parser.add_argument('--source', type=int, required=True, help='Source token index')
    flow_parser.add_argument('--target', type=int, required=True, help='Target token index')
    flow_parser.add_argument('--output', '-o', required=True, help='Output JSON file')

    markov_parser = subparsers.add_parser('markov', help='Compute Markov steady-state distribution')
    markov_parser.add_argument('input', nargs='+', help='Input CSV file(s)')
    markov_parser.add_argument('--layer', type=int, help='Specific layer (default: average all layers)')
    markov_parser.add_argument('--output', '-o', required=True, help='Output JSON file')

    all_parser = subparsers.add_parser('all', help='Compute all metrics')
    all_parser.add_argument('input', nargs='+', help='Input CSV file(s)')
    all_parser.add_argument('--format', default='json', choices=['json', 'csv'],
                           help='Output format (default: json)')
    all_parser.add_argument('--output', '-o', required=True, help='Output file')

    args = parser.parse_args()

    input_files = []
    for pattern in args.input:
        matches = glob.glob(pattern)
        if matches:
            input_files.extend(matches)
        else:
            input_files.append(pattern)

    args.input = input_files

    if args.command == 'entropy':
        cmd_entropy(args)
    elif args.command == 'special-ratio':
        cmd_special_ratio(args)
    elif args.command == 'distance':
        cmd_distance(args)
    elif args.command == 'sparsity':
        cmd_sparsity(args)
    elif args.command == 'flow':
        cmd_flow(args)
    elif args.command == 'markov':
        cmd_markov(args)
    elif args.command == 'all':
        cmd_all(args)

if __name__ == '__main__':
    main()
