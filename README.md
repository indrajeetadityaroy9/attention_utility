# BERT Attention Toolkit
CLI tool for extracting and analyzing attention mechanisms from transformer models (BERT, RoBERTa, DistilBERT, ALBERT, ELECTRA)

- Extract attention weights, Q/K/V matrices, tokens, and attention rollout
- Compute metrics: entropy, sparsity, attention distance, special token ratio, flow, Markov steady-state
- Per-layer/per-head metric breakdowns
